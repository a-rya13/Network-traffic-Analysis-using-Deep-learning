{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffc6907-e77c-47af-800c-d847963076fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['Pkt Len Max_935', 'Fwd Header Len_168', 'Tot Fwd Pkts_5',\n",
      "       'Tot Bwd Pkts_0', 'Bwd Header Len_0', 'Subflow Bwd Pkts_0',\n",
      "       'Fwd Seg Size Min_40', 'Fwd Act Data Pkts_1', 'Init Bwd Win Byts_-1',\n",
      "       'Init Fwd Win Byts_225', 'Subflow Fwd Pkts_5', 'ACK Flag Cnt_1',\n",
      "       'Fwd Header Len_40', 'Fwd Act Data Pkts_0', 'Init Fwd Win Byts_26883',\n",
      "       'Init Bwd Win Byts_211', 'ACK Flag Cnt_0', 'Bwd Header Len_104',\n",
      "       'Pkt Len Max_0', 'Subflow Bwd Pkts_3'],\n",
      "      dtype='object')\n",
      "Epoch 1/20, Loss: 0.3649, Train Acc: 70.15%, Test Acc: 77.75%\n",
      "Epoch 2/20, Loss: 0.3472, Train Acc: 71.17%, Test Acc: 77.75%\n",
      "Epoch 3/20, Loss: 0.3401, Train Acc: 67.96%, Test Acc: 77.76%\n",
      "Epoch 4/20, Loss: 0.3439, Train Acc: 70.47%, Test Acc: 77.76%\n",
      "Epoch 5/20, Loss: 0.3407, Train Acc: 69.62%, Test Acc: 77.76%\n",
      "Epoch 6/20, Loss: 0.3384, Train Acc: 70.01%, Test Acc: 77.76%\n",
      "Epoch 7/20, Loss: 0.3408, Train Acc: 70.28%, Test Acc: 77.76%\n",
      "Early stopping.\n",
      "\n",
      "Final Test Accuracy: 77.76%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"02-16-2018.csv\", low_memory=False)\n",
    "data = data.sample(n=400000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Normalize label values\n",
    "data['Label'] = data['Label'].replace({\n",
    "    'DoS attacks-Hulk': 1,\n",
    "    'DoS attacks-SlowHTTPTest': 2,\n",
    "    'Benign': 0,\n",
    "    'Other': 2\n",
    "})\n",
    "data['Label'] = pd.to_numeric(data['Label'], errors='coerce').fillna(0).astype(int)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "data = data.infer_objects(copy=False)\n",
    "\n",
    "# Inject 10% label noise\n",
    "np.random.seed(42)\n",
    "noise_indices = np.random.choice(len(data), size=int(0.10 * len(data)), replace=False)\n",
    "data.loc[noise_indices, 'Label'] = np.random.randint(0, 3, size=len(noise_indices))\n",
    "\n",
    "# Separate features\n",
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = data.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = [col for col in categorical_features if col != 'Label']\n",
    "numerical_features = [col for col in numerical_features if col != 'Label']\n",
    "\n",
    "# Remove high-cardinality categorical features\n",
    "categorical_features = [col for col in categorical_features if data[col].nunique() <= 100]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "encoded = encoder.fit_transform(data[categorical_features])\n",
    "encoded = encoded.toarray() if encoded.shape[1] < 10000 else encoded\n",
    "\n",
    "if isinstance(encoded, np.ndarray):\n",
    "    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
    "    X = pd.concat([data[numerical_features], encoded_df], axis=1)\n",
    "else:\n",
    "    X = data[numerical_features]\n",
    "y = data['Label']\n",
    "\n",
    "# Feature selection using RandomForest importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)  # Increased n_estimators\n",
    "rf.fit(X.fillna(0), y)\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1][:20]  # Selecting top 20 features\n",
    "selected_feature_names = X.columns[indices]\n",
    "X_selected = X[selected_feature_names]\n",
    "\n",
    "print(\"Selected Features:\", selected_feature_names)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Mixup augmentation\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# Label smoothing\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        log_probs = nn.functional.log_softmax(x, dim=-1)\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
    "\n",
    "# Tensor conversion\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# CNN+LSTM Model with Enhancements\n",
    "class EnhancedCNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(EnhancedCNN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)  # Increased filters\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # Added second convolution layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.3)  # More LSTM layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x[:, -1, :])\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EnhancedCNN_LSTM(X_train_tensor.shape[2], hidden_dim=128, num_classes=3).to(device)  # Larger hidden_dim\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.05)  # Reduced smoothing to 0.05 for less smoothing\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Reduced learning rate\n",
    "\n",
    "# Training\n",
    "best_acc = 0\n",
    "patience = 4\n",
    "counter = 0\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, labels)\n",
    "        outputs = model(inputs)\n",
    "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct_test, total_test = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "    print(f\"Epoch {epoch+1}/20, Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Final Evaluation\n",
    "print(f\"\\nFinal Test Accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f160b-b8f9-4f49-b253-1ba1818b36fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4e5e4-1253-4d6c-a9be-72234dd102f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
